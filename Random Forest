随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而本质属于机器学习的一大分支---集成学习（Ensemble Learning）方法。

随机森林是一种Bagging的方法，首先说明Bagging的主要思想：
1.从包含N个样本的数据集中采用有放回的抽样方式抽取一定数量（可以为N）的样本，构成一个数据集；
2.按照1的方式得到M个数据集
3.利用机器学习的方法（比如SVM,决策树,神经网络...）对得到的M个数据集，训练出M个弱学习模型（比随机猜测好）
4.集成M个弱模型的结果作为最终结果。
注意：由于M个数据集是独立的，因此这M个模型之间也是相互独立的，在最终的集成结果中每个模型的权重是一样的。
这里集成的方式：（1）回归问题：M个模型结果的均值；（2）分类问题：对M个模型的结果进行投票决定，票数多的最为结果，票数一样的随机选择。

随机森林的主要思想：
1.在包含N个样本的数据集中采取有放回的抽样方式 随机 选择N个样本，构成中间数据集，然后在这个数据集的特征维度M中 随机 选取m个特征作为最终的一个数据集；
2.按照1的方式重复进行n次得到n个这样的数据集；
3.利用CART算法对每一个数据集建立一个完全生长的决策树，最终得到n个CART决策树；
4.集成每棵决策树的结果作为最终的结果，对于回归问题用每棵树预测值的平均值，而对于分类问题，对每棵树的结果采用投票计数的方法决定最终的结果；特别地，对于
二分类问题可以把n取做奇数能够避免票数一样的问题。

随机森林分类效果（错误率）与两个因素有关：
1.森林中任意两棵树的相关性：相关性越大，错误率越大；
2.森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
因此减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。
所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。
