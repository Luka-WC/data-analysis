Boosting方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。
主要思想：
1.每一次都根据上一次训练得到的模型结果，调整数据集样本的分布，然后再生成下一个模型；
2.直到生成M个模型；
根据这M个模型的结果集成得到最终的结果。

集成方式：每个模型的重要度作为每个模型结果的权重，然后加权计算得出结果。
Boosting中生成多个模型的方法并不是和Bagging一样并行生成，而是串行生成，因此也决定了多个模型结果的集成是串行集成,也就是每个模型的结果权重并不是一样的。
如何来调整样本分布以及计算模型的重要度，不同方法有不同的定义。

主要方法有：Adaboost    GBDT    XGBoost    LightGBM    CatBoost    
